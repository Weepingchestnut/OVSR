function: 'train' # train, test, get_complexity
#function: 'get_complexity' # train, test, get_complexity

path:
  base: './'
  train: './data/filelist_train.txt'   # use .txt file, or the folder of your data like /xxx/data
  eval: './data/filelist_eval.txt'
#  eval: './data/filelist_eval_vid4.txt'
#  test: '/xxx/data'
  test: '/home/lzk/workspace/OVSR/data/Vid4/calendar/truth'
  checkpoint: 'checkpoint/dis1gpu'
  eval_result: 'eval/dis1gpu'

#gpus: [0, 1]
gpus: [0]
#gpu_id: 0
data_kind: 'single'    # single for hr only, double for lr and hr
data_downsample: 'bi'     # bi: bicubic_pytorch; bd: duf_downsample

train:
  resume: -999           # -999 to load the latest checkpoint
  num_frame: 9           # number of frames for training
  sub_frame: 1           # number of frames at the beginning and the end not used for computing the loss during training
  batch_size: 4
  in_size: 64            # patch_size of LR frames during training
  init_lr: 1.e-3         # initial learning rate, and you may lower it
  final_lr: 1.e-4        # decays to the final learning rate after {epoch_decay} epochs
  epoch_decay: 240       # number of epochs from init_lr to final_lr
  loss: 'cha_loss'       # cha_loss, MSELoss, L1Loss
  alpha: 0.01            # hyper-parameter to adjust the weight of precursor in loss, 0.01 and 0.1 for global and local should be fine
  num_epochs: 1000        # max number of epochs for training
  num_workers: 4
  iter_per_epoch: 500    # iterations per epoch
  display_iter: 20       # iterations to print info
  epoch: 0               # current epoch, updated during training

eval:
  num_workers: 4
  batch_size: 1
  save_all_sr: 0          # 1: save all SR sequence; 0: only save one SR frame

test:
  save_name: 'meta-govsr_4+2_56_bi'       # name to save test results

model:
  file: 'metaovsr'                    # name of model file (models/ovsr.py)
  name: 'meta-govsr_4+2_56'            # name to save/load checkpoint and evaluation, change it for different settings
  kind: 'global'                  # local or global
  meta_upscale: 1                 # 1: use meta-upscale module; 0: use official upscale module
  num_frame: 3                    # we adopt one frame from the past, present, and future respectively
  train_scale: [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0]
  val_scale: 3.5
  num_pb: 4                       # number of residual blocks in the precursor
  num_sb: 2                       # number of residual blocks in the successor
  basic_filter: 56                # number of filters for conv layer